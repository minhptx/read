{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f1b7441-8245-492b-8404-55d9e2384648",
   "metadata": {},
   "outputs": [],
   "source": [
    " from tango import Workspace\n",
    " workspace = Workspace.from_url(\"local:///root/workspace/read/temp/training\")\n",
    " model = workspace.step_result_for_run(\"proud-donkey\", \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64492d51-84d4-4e51-9517-f2f60a97c89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from tango import Step\n",
    "from tango.common.dataset_dict import DatasetDict\n",
    "import pandas as pd\n",
    "from transformers import TapasTokenizer\n",
    "\n",
    "class TableDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ex_id = idx % 2\n",
    "        idx = idx // 2\n",
    "        item = self.df.iloc[idx]\n",
    "        table = pd.DataFrame(json.loads(item[\"table\"]))\n",
    "        cells = zip(*item[\"highlighted_cells\"])\n",
    "        cells = [list(x) for x in cells]\n",
    "        sub_table = table.iloc[cells[0], cells[1]].reset_index().astype(str)\n",
    "\n",
    "        if ex_id == 0:\n",
    "            encoding = self.tokenizer(\n",
    "                table=sub_table,\n",
    "                queries=item[\"positive\"],\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            encoding[\"labels\"] = torch.tensor([1])\n",
    "        else:\n",
    "            encoding = self.tokenizer(\n",
    "                table=sub_table,\n",
    "                queries=item[\"negative\"],\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            encoding[\"labels\"] = torch.tensor([0])\n",
    "\n",
    "        encoding = {key: val[-1] for key, val in encoding.items()}\n",
    "        return encoding\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e95e920-efd9-49d2-8277-bb264ae5326c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TapasTokenizer.from_pretrained(\"google/tapas-base\", max_question_length=256)\n",
    "torch.manual_seed(1)\n",
    "dev_df = pd.read_json(\"../temp/seed/sent_selection/data/dev.jsonl\", lines=True)\n",
    "\n",
    "dev_dataset = TableDataset(dev_df, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04bb3f17-10bc-42a1-b46b-e0f67e27c18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.tapas.modeling_tapas.TapasForSequenceClassification'>\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import accelerate\n",
    "\n",
    "import evaluate\n",
    "name2metrics = {\n",
    "    \"accuracy\": evaluate.load(\"accuracy\"),\n",
    "    \"precision\": evaluate.load(\"precision\"),\n",
    "    \"recall\": evaluate.load(\"recall\"),\n",
    "    \"f1\": evaluate.load(\"f1\"),\n",
    "}\n",
    "\n",
    "dataloader = DataLoader(dev_dataset, batch_size=8, shuffle=False)\n",
    "accelerator = accelerate.Accelerator()\n",
    "\n",
    "print(type(model))\n",
    "\n",
    "model, dataloader = accelerator.prepare(model, dataloader)\n",
    "\n",
    "for batch in dataloader:\n",
    "    y_hat = model(**batch)\n",
    "    preds = y_hat.logits.argmax(dim=1)\n",
    "    for metric in name2metrics.values():\n",
    "        metric.add_batch(predictions=preds, references=batch[\"labels\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23b1a027-2f2b-40b7-ae18-0979d8045859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy {'accuracy': 0.9653325817361894}\n",
      "precision {'precision': 0.97524467472654}\n",
      "recall {'recall': 0.9549041713641488}\n",
      "f1 {'f1': 0.9649672457989177}\n"
     ]
    }
   ],
   "source": [
    "for name, metric in name2metrics.items():\n",
    "    print(name, metric.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34bf70f-5d4b-47b4-a579-4d60d170fb61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
