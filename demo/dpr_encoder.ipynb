{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/read/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DPRContextEncoder, DPRContextEncoderTokenizerFast\n",
    "\n",
    "tokenizer = DPRContextEncoderTokenizerFast.from_pretrained(\"facebook/dpr-ctx_encoder-multiset-base\")\n",
    "model = DPRContextEncoder.from_pretrained(\"../temp/dpr/models/ctx_encoder_dpr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(\"../data/totto2/filtered/train.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "df[\"table_values\"] = df[\"table\"].apply(lambda x: pd.DataFrame(json.loads(x)).values.flatten().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "\n",
    "all_values = list(itertools.chain.from_iterable(df[\"table_values\"].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "sampled_values = random.sample(all_values, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-22 17:33:45,075 https://nlp.informatik.hu-berlin.de/resources/models/ner/en-ner-conll03-v0.4.pt not found in cache, downloading to /tmp/tmphhkznqfi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 432197603/432197603 [01:06<00:00, 6451534.95B/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-22 17:34:52,673 copying /tmp/tmphhkznqfi to cache at /root/.flair/models/en-ner-conll03-v0.4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-22 17:34:52,938 removing temp file /tmp/tmphhkznqfi\n",
      "2022-09-22 17:34:52,975 loading file /root/.flair/models/en-ner-conll03-v0.4.pt\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'KeyedVectors' object has no attribute 'key_to_index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [10], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     sentence \u001b[38;5;241m=\u001b[39m Sentence(value)\n\u001b[0;32m---> 14\u001b[0m     \u001b[43mtagger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m entity \u001b[38;5;129;01min\u001b[39;00m sentence\u001b[38;5;241m.\u001b[39mget_spans(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mner\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     16\u001b[0m         tag \u001b[38;5;241m=\u001b[39m entity\u001b[38;5;241m.\u001b[39mget_label(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mner\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mvalue\n",
      "File \u001b[0;32m/opt/conda/envs/read/lib/python3.8/site-packages/flair/models/sequence_tagger_model.py:369\u001b[0m, in \u001b[0;36mSequenceTagger.predict\u001b[0;34m(self, sentences, mini_batch_size, all_tag_prob, verbose, label_name, return_loss, embedding_storage_mode)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m batch:\n\u001b[1;32m    367\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m--> 369\u001b[0m feature \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(batch)\n\u001b[1;32m    371\u001b[0m \u001b[39mif\u001b[39;00m return_loss:\n\u001b[1;32m    372\u001b[0m     overall_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_calculate_loss(feature, batch)\n",
      "File \u001b[0;32m/opt/conda/envs/read/lib/python3.8/site-packages/flair/models/sequence_tagger_model.py:624\u001b[0m, in \u001b[0;36mSequenceTagger.forward\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m    622\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, sentences: List[Sentence]):\n\u001b[0;32m--> 624\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membeddings\u001b[39m.\u001b[39;49membed(sentences)\n\u001b[1;32m    626\u001b[0m     names \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings\u001b[39m.\u001b[39mget_names()\n\u001b[1;32m    628\u001b[0m     lengths: List[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m [\u001b[39mlen\u001b[39m(sentence\u001b[39m.\u001b[39mtokens) \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m sentences]\n",
      "File \u001b[0;32m/opt/conda/envs/read/lib/python3.8/site-packages/flair/embeddings/token.py:71\u001b[0m, in \u001b[0;36mStackedEmbeddings.embed\u001b[0;34m(self, sentences, static_embeddings)\u001b[0m\n\u001b[1;32m     68\u001b[0m     sentences \u001b[39m=\u001b[39m [sentences]\n\u001b[1;32m     70\u001b[0m \u001b[39mfor\u001b[39;00m embedding \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings:\n\u001b[0;32m---> 71\u001b[0m     embedding\u001b[39m.\u001b[39;49membed(sentences)\n",
      "File \u001b[0;32m/opt/conda/envs/read/lib/python3.8/site-packages/flair/embeddings/base.py:60\u001b[0m, in \u001b[0;36mEmbeddings.embed\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m everything_embedded \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstatic_embeddings:\n\u001b[0;32m---> 60\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_add_embeddings_internal(sentences)\n\u001b[1;32m     62\u001b[0m \u001b[39mreturn\u001b[39;00m sentences\n",
      "File \u001b[0;32m/opt/conda/envs/read/lib/python3.8/site-packages/flair/embeddings/token.py:233\u001b[0m, in \u001b[0;36mWordEmbeddings._add_embeddings_internal\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    231\u001b[0m             word \u001b[39m=\u001b[39m token\u001b[39m.\u001b[39mget_tag(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfield)\u001b[39m.\u001b[39mvalue\n\u001b[0;32m--> 233\u001b[0m         word_embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_cached_vec(word\u001b[39m=\u001b[39;49mword)\n\u001b[1;32m    235\u001b[0m         token\u001b[39m.\u001b[39mset_embedding(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname, word_embedding)\n\u001b[1;32m    237\u001b[0m \u001b[39mreturn\u001b[39;00m sentences\n",
      "File \u001b[0;32m/opt/conda/envs/read/lib/python3.8/site-packages/flair/embeddings/token.py:202\u001b[0m, in \u001b[0;36mWordEmbeddings.get_cached_vec\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[39m@lru_cache\u001b[39m(maxsize\u001b[39m=\u001b[39m\u001b[39m10000\u001b[39m, typed\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    201\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_cached_vec\u001b[39m(\u001b[39mself\u001b[39m, word: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m--> 202\u001b[0m     \u001b[39mif\u001b[39;00m word \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprecomputed_word_embeddings:\n\u001b[1;32m    203\u001b[0m         word_embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecomputed_word_embeddings[word]\n\u001b[1;32m    204\u001b[0m     \u001b[39melif\u001b[39;00m word\u001b[39m.\u001b[39mlower() \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecomputed_word_embeddings:\n",
      "File \u001b[0;32m/opt/conda/envs/read/lib/python3.8/site-packages/gensim/models/keyedvectors.py:650\u001b[0m, in \u001b[0;36mKeyedVectors.__contains__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__contains__\u001b[39m(\u001b[39mself\u001b[39m, key):\n\u001b[0;32m--> 650\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhas_index_for(key)\n",
      "File \u001b[0;32m/opt/conda/envs/read/lib/python3.8/site-packages/gensim/models/keyedvectors.py:647\u001b[0m, in \u001b[0;36mKeyedVectors.has_index_for\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhas_index_for\u001b[39m(\u001b[39mself\u001b[39m, key):\n\u001b[1;32m    639\u001b[0m     \u001b[39m\"\"\"Can this model return a single index for this key?\u001b[39;00m\n\u001b[1;32m    640\u001b[0m \n\u001b[1;32m    641\u001b[0m \u001b[39m    Subclasses that synthesize vectors for out-of-vocabulary words (like\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    645\u001b[0m \n\u001b[1;32m    646\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 647\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_index(key, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/envs/read/lib/python3.8/site-packages/gensim/models/keyedvectors.py:413\u001b[0m, in \u001b[0;36mKeyedVectors.get_index\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_index\u001b[39m(\u001b[39mself\u001b[39m, key, default\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    409\u001b[0m     \u001b[39m\"\"\"Return the integer index (slot/position) where the given key's vector is stored in the\u001b[39;00m\n\u001b[1;32m    410\u001b[0m \u001b[39m    backing vectors array.\u001b[39;00m\n\u001b[1;32m    411\u001b[0m \n\u001b[1;32m    412\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 413\u001b[0m     val \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey_to_index\u001b[39m.\u001b[39mget(key, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    414\u001b[0m     \u001b[39mif\u001b[39;00m val \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    415\u001b[0m         \u001b[39mreturn\u001b[39;00m val\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KeyedVectors' object has no attribute 'key_to_index'"
     ]
    }
   ],
   "source": [
    "labels = []\n",
    "import re\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "tagger = SequenceTagger.load('ner')\n",
    "\n",
    "\n",
    "for value in sampled_values:\n",
    "    if not re.search('[A-Za-z]', value):\n",
    "        labels.append(\"NUM\")\n",
    "    else:\n",
    "        sentence = Sentence(value)\n",
    "        tagger.predict(sentence)\n",
    "        for entity in sentence.get_spans('ner'):\n",
    "            tag = entity.get_label(\"ner\").value\n",
    "            labels.append(\"ENTITY\")\n",
    "            break\n",
    "        else:\n",
    "            labels.append(\"TEXT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('read')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b04cdd20d906f01249004a00e02b15317fd602467e42d5d2658435d7da6fec22"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
